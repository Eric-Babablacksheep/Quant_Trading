{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff2f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Tuple, Callable\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "from dataloader import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87dabe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0e9d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:(36776, 20, 8)\n",
      "X test shape:(15762, 20, 8)\n",
      "y train shape:(36776, 1)\n",
      "y test shape:(15762, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X train shape:{X_train_tensor.shape}\")\n",
    "print(f\"X test shape:{X_test_tensor.shape}\")\n",
    "print(f\"y train shape:{y_train_tensor.shape}\")\n",
    "print(f\"y test shape:{y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e7dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to time our experiments\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ccd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout_prob: float = 0.2):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attn_weight = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),  \n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        device = x.device \n",
    "\n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "\n",
    "        # LSTM output\n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0))  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Apply attention: calculate attention weights\n",
    "        attn_scores = torch.tanh(self.attn_weight(lstm_out))  # [batch, seq_len, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize over seq_len\n",
    "\n",
    "        # Weighted sum of LSTM outputs using attention weights\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # [batch, hidden_size]\n",
    "\n",
    "        # Dropout and output\n",
    "        out = self.dropout(context_vector)\n",
    "        out = self.fc(out)  # [batch, 1]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13068b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Instantiate a sample model for test\n",
    "model = AttentionLSTM(input_size=8, hidden_size=33, num_layers=3, dropout_prob=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484e5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8b4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_fn(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate binary accuracy with threshold 0.5\"\"\"\n",
    "    y_pred_label = (y_pred > 0.5).float()\n",
    "    correct = (y_pred_label == y_true).sum().item()\n",
    "    return correct / len(y_true) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7805a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module,\n",
    "                X_train: torch.Tensor,\n",
    "                y_train: torch.Tensor,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                epochs: int = 100):\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Move data to device\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train).squeeze()  # shape: [batch_size]\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = binary_accuracy_fn(y_train, y_pred)\n",
    "\n",
    "        # Logging\n",
    "        if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.5f} | Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a84178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def recursive_forecast(\n",
    "    model: torch.nn.Module,\n",
    "    initial_input: torch.Tensor,\n",
    "    forecast_steps: int,\n",
    "    lookback: int,\n",
    "    device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Recursively forecasts future values using the trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        initial_input: Tensor of shape [1, lookback, num_features]\n",
    "        forecast_steps: Number of future steps to predict\n",
    "        lookback: Length of input window\n",
    "        device: CPU or CUDA\n",
    "    \n",
    "    Returns:\n",
    "        forecast: Tensor of shape [forecast_steps, output_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecast = []\n",
    "\n",
    "    current_input = initial_input.clone().to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(forecast_steps):\n",
    "            pred = model(current_input)  # [1, 1]\n",
    "            forecast.append(pred.squeeze(0))  # remove batch dim\n",
    "\n",
    "            # Update input for next prediction\n",
    "            last_features = current_input[:, -1, :].clone()\n",
    "            last_features[:, 0] = pred.squeeze(1)\n",
    "            new_step = last_features.unsqueeze(1)\n",
    "            current_input = torch.cat([current_input[:, 1:], new_step], dim=1)\n",
    "\n",
    "    return torch.stack(forecast)  # [forecast_steps, output_dim]\n",
    "\n",
    "\n",
    "def evaluate_forecast(\n",
    "    forecast: torch.Tensor,\n",
    "    ground_truth: torch.Tensor,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    metrics_fn: Callable[[torch.Tensor, torch.Tensor], float] = None\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compares forecast with ground truth and computes evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        forecast: Tensor of predicted values [forecast_steps, output_dim]\n",
    "        ground_truth: Tensor of true values [forecast_steps, output_dim]\n",
    "        loss_fn: Loss function (e.g. MSE)\n",
    "        metrics_fn: Optional function to compute additional metrics like R^2\n",
    "\n",
    "    Returns:\n",
    "        loss_value, metrics_value\n",
    "    \"\"\"\n",
    "    loss_value = loss_fn(forecast, ground_truth).item()\n",
    "    metrics_value = metrics_fn(forecast, ground_truth).item() if metrics_fn else None\n",
    "    return loss_value, metrics_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "\n",
    "def regression_r2(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"R² score metric for regression\"\"\"\n",
    "    return torch.tensor(r2_score(y_true.cpu().numpy(), y_pred.cpu().numpy()))\n",
    "\n",
    "def run_training_and_testing(\n",
    "    model: torch.nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    lookback: int,\n",
    "    device: torch.device,\n",
    "    epochs: int = 100,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    # Define loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Begin training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train).squeeze()\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate every 20 epochs\n",
    "        if epoch % 20 == 0 or epoch == epochs:\n",
    "            # Forecast using recursive strategy\n",
    "            initial_input = X_test[0].unsqueeze(0)  # [1, lookback, features]\n",
    "            forecast_steps = len(y_test)\n",
    "            forecast = recursive_forecast(model, initial_input, forecast_steps, lookback, device)\n",
    "\n",
    "            # Evaluate forecast\n",
    "            test_loss, test_r2 = evaluate_forecast(forecast, y_test[:forecast_steps], loss_fn, regression_r2)\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            print(f\"  Train Loss: {loss.item():.5f}\")\n",
    "            print(f\"  Test Loss:  {test_loss:.5f} | R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481d282",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "print(x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d2c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m run_training_and_testing(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mAttentionLSTM(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      5\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train_tensor,\n\u001b[0;32m      6\u001b[0m     y_train\u001b[38;5;241m=\u001b[39my_train_tensor,\n\u001b[0;32m      7\u001b[0m     X_test\u001b[38;5;241m=\u001b[39mX_test_tensor,\n\u001b[0;32m      8\u001b[0m     y_test\u001b[38;5;241m=\u001b[39my_test_tensor,\n\u001b[0;32m      9\u001b[0m     lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     10\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mrun_training_and_testing\u001b[1;34m(model, X_train, y_train, X_test, y_test, lookback, device, epochs, lr)\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_train)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mAttentionLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# x: [batch_size, seq_len, input_size]\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Initialize hidden state\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     h_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_model = run_training_and_testing(\n",
    "    model=AttentionLSTM(input_size=8, hidden_size=64, num_layers=2),\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    lookback=20,\n",
    "    device=device,\n",
    "    epochs=100\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
