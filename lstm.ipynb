{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df586047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Tuple, Callable\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "from dataloader import load_dataset, get_cryptoquant_data, combineTA_to_OHLCV\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a833f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        close        volume       ema  rsi  MACD_12_26_9  \\\n",
      "datetime                                                                   \n",
      "2019-04-14 02:00:00  0.325129  2.390760e+06       NaN  NaN           NaN   \n",
      "2019-04-14 03:00:00  0.325212  2.035724e+06       NaN  NaN           NaN   \n",
      "2019-04-14 04:00:00  0.325642  1.560139e+06       NaN  NaN           NaN   \n",
      "2019-04-14 05:00:00  0.323081  2.632353e+06       NaN  NaN           NaN   \n",
      "2019-04-14 06:00:00  0.322276  2.753541e+06       NaN  NaN           NaN   \n",
      "2019-04-14 07:00:00  0.323365  1.710360e+06       NaN  NaN           NaN   \n",
      "2019-04-14 08:00:00  0.323134  1.605113e+06       NaN  NaN           NaN   \n",
      "2019-04-14 09:00:00  0.323954  2.177065e+06       NaN  NaN           NaN   \n",
      "2019-04-14 10:00:00  0.325225  1.280122e+06       NaN  NaN           NaN   \n",
      "2019-04-14 11:00:00  0.324816  1.051731e+06  0.324183  NaN           NaN   \n",
      "\n",
      "                     MACDh_12_26_9  MACDs_12_26_9  bollinger_width  \n",
      "datetime                                                            \n",
      "2019-04-14 02:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 03:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 04:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 05:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 06:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 07:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 08:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 09:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 10:00:00            NaN            NaN              NaN  \n",
      "2019-04-14 11:00:00            NaN            NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "data = get_cryptoquant_data()\n",
    "dataf = combineTA_to_OHLCV(data).process()\n",
    "print(dataf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99679833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_tensor, y_train_tensor, y_test_tensor, data_min, data_max, scaler = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e1c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:(36753, 20, 8)\n",
      "X test shape:(15752, 20, 8)\n",
      "y train shape:(36753, 8)\n",
      "y test shape:(15752, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X train shape:{X_train_tensor.shape}\")\n",
    "print(f\"X test shape:{X_test_tensor.shape}\")\n",
    "print(f\"y train shape:{y_train_tensor.shape}\")\n",
    "print(f\"y test shape:{y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e86fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to time our experiments\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebdea0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout_prob: float = 0.2):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attn_weight = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Output layer: now outputs 8 features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),  # output 8 features\n",
    "            # nn.Sigmoid()  # Uncomment this ONLY if you want outputs in range [0, 1]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        # LSTM output\n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0))  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = torch.tanh(self.attn_weight(lstm_out))  # [batch, seq_len, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # [batch, seq_len, 1]\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  # [batch, hidden_size]\n",
    "\n",
    "        # Dropout + final output\n",
    "        out = self.dropout(context_vector)\n",
    "        out = self.fc(out)  # [batch, 8]\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Bayesian Optimization\n",
    "def objective(params):\n",
    "    num_layers = int(params[0])\n",
    "    hidden_size = int(params[1])\n",
    "    learning_rate = params[2]\n",
    "    dropout_prob = params[3]\n",
    "    optimizer_name = params[4]\n",
    "    num_epochs = params[5]\n",
    "\n",
    "    # Instantiate the CNN-LSTM model with these hyperparameters\n",
    "    model = AttentionLSTM(input_size=8, hidden_size=hidden_size, num_layers=num_layers, dropout_prob=dropout_prob)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop \n",
    "    model.train()\n",
    "    num_epochs = num_epochs\n",
    "    batch_size = 30\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            inputs = X_train_tensor[i:i+batch_size]\n",
    "            labels = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_loss = loss_fn(val_outputs, y_test_tensor)\n",
    "\n",
    "    return val_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a43960",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Instantiate a sample model for test\n",
    "model = AttentionLSTM(input_size=8, hidden_size=33, num_layers=3, dropout_prob=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b461dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b912f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_r2(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    y_true_np = y_true.cpu().numpy()\n",
    "    y_pred_np = y_pred.cpu().numpy()\n",
    "\n",
    "    # Compute R² for each feature separately and average\n",
    "    r2_scores = [r2_score(y_true_np[:, i], y_pred_np[:, i]) for i in range(y_true_np.shape[1])]\n",
    "    return torch.tensor(sum(r2_scores) / len(r2_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0928f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def recursive_forecast(\n",
    "    model: torch.nn.Module,\n",
    "    initial_input: torch.Tensor,\n",
    "    forecast_steps: int,\n",
    "    lookback: int,\n",
    "    device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Recursively forecasts future multi-feature values using the trained LSTM model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        initial_input: Tensor of shape [1, lookback, num_features]\n",
    "        forecast_steps: Number of future steps to predict\n",
    "        lookback: Length of input window\n",
    "        device: CPU or CUDA\n",
    "\n",
    "    Returns:\n",
    "        forecast: Tensor of shape [forecast_steps, output_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecast = []\n",
    "\n",
    "    current_input = initial_input.clone().to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(forecast_steps):\n",
    "            pred = model(current_input)  # [1, output_dim] -> [1, 8]\n",
    "            forecast.append(pred.squeeze(0))  # [8]\n",
    "\n",
    "            # Replace last step with predicted features\n",
    "            new_step = pred.unsqueeze(1)  # [1, 1, 8]\n",
    "            current_input = torch.cat([current_input[:, 1:], new_step], dim=1)  # slide window\n",
    "\n",
    "    return torch.stack(forecast)  # [forecast_steps, 8]\n",
    "\n",
    "\n",
    "def evaluate_forecast(\n",
    "    forecast: torch.Tensor,\n",
    "    ground_truth: torch.Tensor,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    metrics_fn: Callable[[torch.Tensor, torch.Tensor], float] = None\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compares forecast with ground truth and computes evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        forecast: Tensor of predicted values [forecast_steps, output_dim]\n",
    "        ground_truth: Tensor of true values [forecast_steps, output_dim]\n",
    "        loss_fn: Loss function (e.g. MSE)\n",
    "        metrics_fn: Optional function to compute additional metrics like R^2\n",
    "\n",
    "    Returns:\n",
    "        loss_value, metrics_value\n",
    "    \"\"\"\n",
    "    loss_value = loss_fn(forecast, ground_truth).item()\n",
    "    metrics_value = metrics_fn(forecast, ground_truth).item() if metrics_fn else None\n",
    "    return loss_value, metrics_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94f4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "\n",
    "def run_training_and_testing(\n",
    "    model: torch.nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    lookback: int,\n",
    "    device: torch.device,\n",
    "    epochs: int = 100,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Begin training loop\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred_train = model(X_train)\n",
    "        loss = loss_fn(y_pred_train, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate every 20 epochs\n",
    "        if epoch % 20 == 0 or epoch == epochs:\n",
    "            # Forecast using recursive strategy\n",
    "            initial_input = X_test[0].unsqueeze(0)  # [1, lookback, features]\n",
    "            forecast_steps = len(y_test)\n",
    "            y_pred_test = recursive_forecast(model, initial_input, forecast_steps, lookback, device)\n",
    "\n",
    "            # Evaluate y_pred_test\n",
    "            test_loss, test_r2 = evaluate_forecast(y_pred_test, y_test[:forecast_steps], loss_fn, regression_r2)\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            print(f\"  Train Loss: {loss.item():.5f}\")\n",
    "            print(f\"  Test Loss:  {test_loss:.5f} | R² Score: {test_r2:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d393344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c28b83219e4024926f33b72331b8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]\n",
      "  Train Loss: 0.01016\n",
      "  Test Loss:  0.02431 | R² Score: -307882575.8647\n",
      "Epoch [40/100]\n",
      "  Train Loss: 0.00706\n",
      "  Test Loss:  0.02293 | R² Score: -36042974.5470\n",
      "Epoch [60/100]\n",
      "  Train Loss: 0.00630\n",
      "  Test Loss:  0.02263 | R² Score: -122572244.8929\n",
      "Epoch [80/100]\n",
      "  Train Loss: 0.00598\n",
      "  Test Loss:  0.02326 | R² Score: -347463083.5751\n",
      "Epoch [100/100]\n",
      "  Train Loss: 0.00561\n",
      "  Test Loss:  0.02317 | R² Score: -144723069.6040\n",
      "Train time on cpu: 500.556 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_0 = timer()\n",
    "\n",
    "trained_model = run_training_and_testing(\n",
    "    model=AttentionLSTM(input_size=8, hidden_size=64, num_layers=2),\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    lookback=20,\n",
    "    device=device,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "train_time_end_model_0 = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_model_0,\n",
    "                                            end=train_time_end_model_0,\n",
    "                                            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41ee242",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1=scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3389db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenormalize\u001b[39m(data: torch\u001b[38;5;241m.\u001b[39mTensor, scaler) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Denormalizes a tensor using the given scaler.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        Denormalized NumPy array\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scaler\u001b[38;5;241m.\u001b[39minverse_transform(data\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def denormalize(data: torch.Tensor, scaler:MinMaxScaler) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Denormalizes a tensor using the given scaler.\n",
    "    \n",
    "    Args:\n",
    "        data: Tensor of shape [N, features]\n",
    "        scaler: Fitted sklearn MinMaxScaler\n",
    "    \n",
    "    Returns:\n",
    "        Denormalized NumPy array\n",
    "    \"\"\"\n",
    "    return scaler.inverse_transform(data.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ce1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize true and predicted values\n",
    "y_train_true_denorm = denormalize(y_train_tensor, scaler)\n",
    "y_train_pred_denorm = denormalize(y_train_pred, scaler)\n",
    "\n",
    "y_test_true_denorm = denormalize(y_test_tensor, scaler)\n",
    "y_test_pred_denorm = denormalize(y_test_pred, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51668302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(true_vals, predicted_vals, title='Prediction vs Actual', feature_idx=0, dataset_type='Train'):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(true_vals[:, feature_idx], label='True', color='blue')\n",
    "    plt.plot(predicted_vals[:, feature_idx], label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'{title} - {dataset_type} Set (Feature {feature_idx})')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot for 'close' price (assuming it's feature 0)\n",
    "plot_predictions(y_train_true_denorm, y_train_pred_denorm, dataset_type='Train', feature_idx=0)\n",
    "plot_predictions(y_test_true_denorm, y_test_pred_denorm, dataset_type='Test', feature_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9dabd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
